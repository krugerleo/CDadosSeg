{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cópia de datascience.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krugerleo/CDadosSeg/blob/master/Final/datascience.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy8ZdeQR2H4p"
      },
      "source": [
        "# Introdução\r\n",
        "## Autores: [Leonardo Krüger](https://github.com/krugerleo/CDadosSeg),[Lucas Block](https://github.com/lucasvillatore/CDadosSeg).[João Picolo](https://github.com/JoaoPicolo/CDadosSeg)\r\n",
        "### Algoritmos utilizados\r\n",
        "- K-nearest neighbors\r\n",
        "- Random Forests\r\n",
        "- Multilayer perceptron\r\n",
        "\r\n",
        "### Dataset Utilizado e informações\r\n",
        "##### Foi utilizado o dataset relacionado a ataque de negação de serviço (DDoS) Esse dataset é disponibilizado nesse link: [ddos-dataset](https://www.kaggle.com/devendra416/ddos-datasets)\r\n",
        "##### O dataset foi gerado e rotulado automaticamente utilizando [CICFlow](https://www.unb.ca/cic/research/applications.html#CICFlowMeter), desta forma é possível a extração de estatísticas sobre características de tráfico de rede.\r\n",
        "\r\n",
        "\r\n",
        "### Campos utilizados\r\n",
        "- timestamp\r\n",
        "- fwd seg size min\r\n",
        "- source ip\r\n",
        "- dst ip\r\n",
        "- flow iat min\r\n",
        "- source port\r\n",
        "- tot fwd pkts\r\n",
        "- init bwd win bytes\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNIFzsyuQt0o"
      },
      "source": [
        "# Instalação\r\n",
        "\r\n",
        "> Para execução correta:\r\n",
        "\r\n",
        "1.   Deve montar o drive com a celula abaixo.\r\n",
        "2.   O **final_dataset.csv** (nome deve ser igual) precisa estar na pasta raiz do drive.\r\n",
        "3.   Para primeira execução não se existe dataset montado então essa opção será \"não\".\r\n",
        "4.   Para futuras execuções é recomendado salvar os fragmentos do dataset que serão gerado e poder utilizar a opção de dataset montado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx0699RtFHZA"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kGOPiqTQGd1"
      },
      "source": [
        "use_mounted = input(\"Use mounted dataset? [y/n] \")\n",
        "\n",
        "if(use_mounted.lower() == 'y'):\n",
        "  phase_one, phase_two = loadDatasets()\n",
        "else:\n",
        "  phase_one, phase_two = mountDatasets()\n",
        "  \n",
        "  save_df = input(\"Save created datasets? [y/n]\")\n",
        "  \n",
        "  if(save_df.lower() == 'y'):\n",
        "    saveDatasets(phase_one, phase_two)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUIPhmtS7e0q"
      },
      "source": [
        "# Import's e definição de variaveis\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAZzMkdN6WGs"
      },
      "source": [
        "import sys\r\n",
        "\r\n",
        "import math\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.pyplot as plt; plt.rcdefaults()\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\r\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, OneHotEncoder\r\n",
        "from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error, plot_roc_curve\r\n",
        "from sklearn.utils import shuffle\r\n",
        "from sklearn import preprocessing\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBXuwPmfVi8B"
      },
      "source": [
        "# Global variables\n",
        "path = '/content/drive/MyDrive/'\n",
        "columns_df = ['Timestamp', 'Fwd Seg Size Min', \"Src IP\", \"Dst IP\", 'Flow IAT Min', 'Src Port', 'Tot Fwd Pkts', 'Flow Duration', 'Label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3MBqoLEB5vM"
      },
      "source": [
        "# Separação e montagem de dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyS-yBQHVfHd"
      },
      "source": [
        "def loadDatasets():\n",
        "  global path, columns_df\n",
        "\n",
        "  print(\"Loading datasets...\")\n",
        "  \n",
        "  \n",
        "  train_path_df = path + \"train_dataset_phase_one.csv\"\n",
        "  train_df = pd.read_csv(\n",
        "      train_path_df,\n",
        "      usecols=columns_df\n",
        "  )\n",
        "\n",
        "  test_path_df = path + \"test_dataset_phase_one.csv\"\n",
        "  test_df = pd.read_csv(\n",
        "      test_path_df,\n",
        "      usecols=columns_df\n",
        "  )\n",
        "\n",
        "  phase_one = {\n",
        "      'train': train_df,\n",
        "      'test': test_df\n",
        "  }\n",
        "\n",
        "  test_path_df = path + \"test_dataset_phase_two.csv\"\n",
        "  test_df = pd.read_csv(\n",
        "      test_path_df,\n",
        "      usecols=columns_df\n",
        "  )\n",
        "\n",
        "  phase_two = {\n",
        "      'test':test_df\n",
        "  }\n",
        "\n",
        "  print(\"Datasets recovered\")\n",
        "\n",
        "  return phase_one, phase_two\n",
        "\n",
        "\n",
        "def mountDatasets():\n",
        "  global path, columns_df\n",
        "\n",
        "  print(\"Creating dataset...\")\n",
        "  path_df = path + \"final_dataset.csv\"\n",
        "\n",
        "  data_frame = pd.read_csv(\n",
        "      path_df,\n",
        "      usecols=columns_df\n",
        "  )\n",
        "  data_frame.describe()\n",
        "  # Separate df by type\n",
        "  data_frame = shuffle(data_frame)\n",
        "  ddos_df = data_frame.loc[data_frame['Label'] == 'ddos']\n",
        "  benign_df = data_frame.loc[data_frame['Label'] == 'Benign']\n",
        "\n",
        "  # Split dfs into training and test dfs\n",
        "  eighty_ddos_percent, twenty_ddos_percent = train_test_split(ddos_df, test_size=0.2)\n",
        "  eighty_ddos_percent, twenty_ddos_percent = train_test_split(twenty_ddos_percent, test_size=0.2)\n",
        "  ddos_train, ddos_test = train_test_split(eighty_ddos_percent, test_size=0.2)\n",
        "  \n",
        "  \n",
        "  eighty_benign_percent, twenty_benign_percent = train_test_split(benign_df, test_size=0.2)\n",
        "  eighty_benign_percent, twenty_benign_percent = train_test_split(twenty_benign_percent, test_size=0.2)\n",
        "  benign_train, benign_test = train_test_split(eighty_benign_percent, test_size=0.2)\n",
        "\n",
        "  # Concatenate dataframes for train and test 80% percent\n",
        "  train_df = pd.concat([ddos_train, benign_train])\n",
        "  test_df = pd.concat([ddos_test, benign_test])\n",
        "  \n",
        "  phase_two_test = pd.concat([twenty_ddos_percent, twenty_benign_percent])\n",
        "  print(\"Dataset created\")\n",
        "\n",
        "  # phase one\n",
        "  phase_one = {\n",
        "      'train': train_df,\n",
        "      'test': test_df\n",
        "  }\n",
        "\n",
        "  phase_two = {\n",
        "      'test': phase_two_test\n",
        "  }\n",
        "\n",
        "  return phase_one, phase_two, \n",
        "\n",
        "\n",
        "def saveDatasets(phase_one, phase_two):\n",
        "  global path\n",
        "\n",
        "  print(\"Saving datasets...\")\n",
        "\n",
        "  phase_one['train'].to_csv(path + \"train_dataset_phase_one.csv\", index=False)\n",
        "  phase_one['test'].to_csv(path + \"test_dataset_phase_one.csv\", index=False)\n",
        "\n",
        "  phase_two['test'].to_csv(path + \"test_dataset_phase_two.csv\", index=False)\n",
        "\n",
        "  print(\"Datasets saved\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL0Oay2SQ8vF"
      },
      "source": [
        "# Dataframes\r\n",
        "\r\n",
        "**train_df_one:** Porcentagem para treino -> 80% de x% do dataset original  \r\n",
        "**test_df_one:** Porcentagem para teste -> 20% de x% do dataset original  \r\n",
        "**test_df_two:** Porcentagem para teste na segunda fase -> 20% de x% do dataset original  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDx6-AccSNVF"
      },
      "source": [
        "train_df_one = phase_one['train']\n",
        "test_df_one = phase_one['test']\n",
        "test_df_two = phase_two['test'] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpF9PjAP9R6y"
      },
      "source": [
        "# Distribuição dataframes\r\n",
        "  Distribuição dos dataframes de teste e treino pelo label [\"ddos\", \"Benign\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7jUon___5sU"
      },
      "source": [
        "## 1 Treino"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szXnqWiW9jrp"
      },
      "source": [
        "ddos_df = train_df_one.loc[train_df_one['Label'] == 'ddos']\r\n",
        "benign_df = train_df_one.loc[train_df_one['Label'] == 'Benign']\r\n",
        "\r\n",
        "objects = (\"Ataque DDoS\", \"Benigno\")\r\n",
        "y_pos = np.arange(len(objects))\r\n",
        "distribution = [len(ddos_df), len(benign_df)]\r\n",
        "\r\n",
        "plt.bar(y_pos, distribution, align='center', alpha=0.5)\r\n",
        "plt.xticks(y_pos, objects)\r\n",
        "plt.ylabel('Quantidade')\r\n",
        "plt.title('Distribuição de amostras no treino')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUw4D3DA_-ul"
      },
      "source": [
        "## 2 Teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzMWfxP5SPUR"
      },
      "source": [
        "ddos_df = test_df_one.loc[test_df_one['Label'] == 'ddos']\n",
        "benign_df = test_df_one.loc[test_df_one['Label'] == 'Benign']\n",
        "\n",
        "objects = (\"Ataque DDoS\", \"Benigno\")\n",
        "y_pos = np.arange(len(objects))\n",
        "distribution = [len(ddos_df), len(benign_df)]\n",
        "\n",
        "plt.bar(y_pos, distribution, align='center', alpha=0.5)\n",
        "plt.xticks(y_pos, objects)\n",
        "plt.ylabel('Quantidade')\n",
        "plt.title('Distribuição de amostras no teste')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdFUeNMo_XGN"
      },
      "source": [
        "#  Algoritmos\r\n",
        " Utilizados com cross validation k-folders = 5\r\n",
        "1.   K-nearest neighbors\r\n",
        "2.   Random Forests\r\n",
        "3.   Multilayer perceptron\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pBeGgUC4Csy"
      },
      "source": [
        "## 1.1 KNeighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avapzr_2cksA"
      },
      "source": [
        "print(\"Executing KNN classification\")\r\n",
        "\r\n",
        "fit_train = train_df_one.apply(LabelEncoder().fit_transform)\r\n",
        "fit_test = test_df_one.apply(LabelEncoder().fit_transform)\r\n",
        "\r\n",
        "n_columns = len(fit_train.columns)\r\n",
        "\r\n",
        "x_train = fit_train.iloc[:, :-1].values # Features ?\r\n",
        "y_train = fit_train.iloc[:, n_columns-1].values # (n_columns-1) = Labels\r\n",
        "\r\n",
        "x_test = fit_test.iloc[:, :-1].values # Features ?\r\n",
        "y_test = fit_test.iloc[:, n_columns-1].values # (n_columns-1) = Labels\r\n",
        "\r\n",
        "# Executes algorithm\r\n",
        "classifier = KNeighborsClassifier(n_neighbors=5)\r\n",
        "classifier.fit(x_train, y_train)\r\n",
        "y_pred = classifier.predict(x_test)\r\n",
        "\r\n",
        "print(confusion_matrix(y_test, y_pred))\r\n",
        "print(mean_absolute_error(y_test, y_pred))\r\n",
        "print(classification_report(y_test, y_pred))\r\n",
        "plot_roc_curve(classifier, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEBQKt974RfT"
      },
      "source": [
        "## 1.2 KNeighbors cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaobzQatZ4UC"
      },
      "source": [
        "print(\"Cross validating KNNN\")\r\n",
        "\r\n",
        "fit_train = train_df_one.apply(LabelEncoder().fit_transform)\r\n",
        "\r\n",
        "n_columns = len(fit_train.columns)\r\n",
        "x = fit_train.iloc[:, :-1].values # Features\r\n",
        "y = fit_train.iloc[:, n_columns-1].values # Labels\r\n",
        "\r\n",
        "\r\n",
        "kf = StratifiedKFold(n_splits=5)\r\n",
        "\r\n",
        "for train_index, test_index in kf.split(x, y):\r\n",
        "  x_train, x_test = x[train_index], x[test_index]\r\n",
        "  y_train, y_test = y[train_index], y[test_index]\r\n",
        "\r\n",
        "  classifier = KNeighborsClassifier(n_neighbors=5)\r\n",
        "  classifier.fit(x_train, y_train)\r\n",
        "  y_pred = classifier.predict(x_test)\r\n",
        "\r\n",
        "\r\n",
        "  print(confusion_matrix(y_test, y_pred))\r\n",
        "  print(mean_absolute_error(y_test, y_pred))\r\n",
        "  print(classification_report(y_test, y_pred))   \r\n",
        "  plot_roc_curve(classifier, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV7N_D5s8NUV"
      },
      "source": [
        "## 2.1 Radom forests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUohAwBi4Y-5"
      },
      "source": [
        "print(\"Executing Random Forest classification\")\r\n",
        "\r\n",
        "fit_train = train_df_one.apply(LabelEncoder().fit_transform)\r\n",
        "fit_test = test_df_one.apply(LabelEncoder().fit_transform)\r\n",
        "\r\n",
        "n_columns = len(fit_train.columns)\r\n",
        "\r\n",
        "x_train = fit_train.iloc[:, :-1].values # Features\r\n",
        "y_train = fit_train.iloc[:, n_columns-1].values # Labels\r\n",
        "\r\n",
        "x_test = fit_test.iloc[:, :-1].values # Features\r\n",
        "y_test = fit_test.iloc[:, n_columns-1].values # Labels\r\n",
        "\r\n",
        "classifier = RandomForestClassifier(n_estimators=50)\r\n",
        "classifier.fit(x_train, y_train)\r\n",
        "y_pred = classifier.predict(x_test)\r\n",
        "\r\n",
        "print(confusion_matrix(y_test, y_pred))\r\n",
        "print(mean_absolute_error(y_test, y_pred))\r\n",
        "print(classification_report(y_test, y_pred))\r\n",
        "plot_roc_curve(classifier, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M9KwaqR8WLN"
      },
      "source": [
        "## 2.2 Random forests cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_eTWWxzAre3"
      },
      "source": [
        "print(\"Cross validating Random Forest\")\r\n",
        "\r\n",
        "fit_train = train_df_one.apply(LabelEncoder().fit_transform)\r\n",
        "\r\n",
        "n_columns = len(fit_train.columns)\r\n",
        "x = fit_train.iloc[:, :-1].values # Features\r\n",
        "y = fit_train.iloc[:, n_columns-1].values # Labels\r\n",
        "\r\n",
        "idx = 0\r\n",
        "kf = StratifiedKFold(n_splits=5)\r\n",
        "\r\n",
        "for train_index, test_index in kf.split(x, y):\r\n",
        "  x_train, x_test = x[train_index], x[test_index]\r\n",
        "  y_train, y_test = y[train_index], y[test_index]\r\n",
        "  idx += 1\r\n",
        "  print(f\"Fold {idx}\\n\")\r\n",
        "\r\n",
        "\r\n",
        "  classifier = RandomForestClassifier(n_estimators=50)\r\n",
        "  classifier.fit(x_train, y_train)\r\n",
        "  y_pred = classifier.predict(x_test)\r\n",
        "\r\n",
        "  print(confusion_matrix(y_test, y_pred))\r\n",
        "  print(mean_absolute_error(y_test, y_pred))\r\n",
        "  print(classification_report(y_test, y_pred))\r\n",
        "  plot_roc_curve(classifier, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg_25nB98qtk"
      },
      "source": [
        "## 3.1 Multilayer perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP7FT10PPesh"
      },
      "source": [
        "executeMLP(train_df_one, test_df_one)\r\n",
        "print(\"Executing MLP classification\")\r\n",
        "\r\n",
        "fit_train = train_df_one.apply(LabelEncoder().fit_transform)\r\n",
        "fit_test = test_df_one.apply(LabelEncoder().fit_transform)\r\n",
        "\r\n",
        "n_columns = len(fit_train.columns)\r\n",
        "\r\n",
        "x_train = fit_train.iloc[:, :-1].values # Features\r\n",
        "y_train = fit_train.iloc[:, n_columns-1].values # Labels\r\n",
        "\r\n",
        "x_test = fit_test.iloc[:, :-1].values # Features\r\n",
        "y_test = fit_test.iloc[:, n_columns-1].values # Labels\r\n",
        "\r\n",
        "# Executes algorithm\r\n",
        "classifier = MLPClassifier(hidden_layer_sizes=6)\r\n",
        "classifier.fit(x_train, y_train)\r\n",
        "y_pred = classifier.predict(x_test)\r\n",
        "\r\n",
        "print(confusion_matrix(y_test, y_pred))\r\n",
        "print(mean_absolute_error(y_test, y_pred))\r\n",
        "print(classification_report(y_test, y_pred))\r\n",
        "plot_roc_curve(classifier, x_test, y_test)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhFfEwit8qX7"
      },
      "source": [
        "## 3.2 Multilayer perceptron cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHgDdSVePU6Q"
      },
      "source": [
        "print(\"Cross validating MLP\")\r\n",
        "\r\n",
        "fit_train = train_df_one.apply(LabelEncoder().fit_transform)\r\n",
        "\r\n",
        "n_columns = len(fit_train.columns)\r\n",
        "x = fit_train.iloc[:, :-1].values # Features\r\n",
        "y = fit_train.iloc[:, n_columns-1].values # Labels\r\n",
        "\r\n",
        "idx = 0\r\n",
        "kf = StratifiedKFold(n_splits=5)\r\n",
        "\r\n",
        "for train_index, test_index in kf.split(x, y):\r\n",
        "  x_train, x_test = x[train_index], x[test_index]\r\n",
        "  y_train, y_test = y[train_index], y[test_index]\r\n",
        "  idx += 1\r\n",
        "  print(f\"Fold {idx}\\n\")\r\n",
        "\r\n",
        "\r\n",
        "  classifier = MLPClassifier(hidden_layer_sizes=6)\r\n",
        "  classifier.fit(x_train, y_train)\r\n",
        "  y_pred = classifier.predict(x_test)\r\n",
        "\r\n",
        "  print(confusion_matrix(y_test, y_pred))\r\n",
        "  print(mean_absolute_error(y_test, y_pred))\r\n",
        "  print(classification_report(y_test, y_pred))\r\n",
        "  plot_roc_curve(classifier, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}